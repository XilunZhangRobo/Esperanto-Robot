{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/llavaft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/et/miniconda3/envs/llavaft/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/et/miniconda3/envs/llavaft/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 2.0e-05,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }\n",
    "\n",
    "peft_config = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\"\n",
    "]\n",
    "}\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n",
      "generation_config.json: 100%|██████████| 172/172 [00:00<00:00, 871kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# checkpoint_path = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 3,822,652,416 || trainable%: 0.041145880630335606\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, peft_conf)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prompt_path = \"/mnt/datascience1/all_data.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "train = prompt[:620]\n",
    "test = prompt[620:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data = [\n",
    "    {\n",
    "    \"text\":\"<s><|user|>\\n\" + row_dict['input'].replace('\\n\\n','\\n').replace('<|user|>\\n', '').replace('\\n<|end|>', '<|end|>') + '\\n<|assistant|>\\n' + row_dict['output'].replace('\\n\\n\\n','\\n').replace('\\n\\n','\\n').replace('<|assistant|>','') + '<|end|>'\n",
    "    }\n",
    "    for row_dict in train# .to_dict(orient=\"records\")\n",
    "]\n",
    "with open(\"/mnt/datascience1/train.json\", \"w\") as f:\n",
    "   json.dump(dataset_data, f)\n",
    "\n",
    "dataset_data = [\n",
    "    {\n",
    "    \"text\":\"<s><|user|>\\n\" + row_dict['input'].replace('\\n\\n','\\n').replace('Starts<', '').replace('>Ends', '').replace('<|user|>\\n', '').replace('\\n<|end|>', '<|end|>') + '\\n<|assistant|>\\n' + row_dict['output'].replace('Starts<', '').replace('>Ends', '').replace('\\n\\n\\n','\\n').replace('\\n\\n','\\n').replace('<|assistant|>','') + '<|end|>'\n",
    "    }\n",
    "    for row_dict in test# .to_dict(orient=\"records\")\n",
    "]\n",
    "with open(\"/mnt/datascience1/test.json\", \"w\") as f:\n",
    "   json.dump(dataset_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 620\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 72\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "final_dataset = load_dataset(\"json\", data_files=\"/mnt/datascience1/train.json\")\n",
    "final_dataset1 = load_dataset(\"json\", data_files=\"/mnt/datascience1/test.json\")\n",
    "final_dataset['train'],final_dataset1['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|user|>\n",
      "Task is move BallG to the center of BoxH\n",
      "You are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\n",
      "Please produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\n",
      "When generating plans, consider spatial relationships meticulously. \n",
      "For example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\n",
      "Here is an example snippet for your reference, demonstrating how to call the function:\n",
      "\"\"\n",
      "python\n",
      "import numpy as np  # import numpy because we are using it below\n",
      "open_gripper()\n",
      "close_gripper()\n",
      "# Get the graspable point of cubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "# Get size of cubeA\n",
      "cubeA_size = get_size('cubeA')\n",
      "move_to_position(cubeA_graspable_point)\n",
      "\"\"\n",
      "Please generate a step-by-step plan followed by a single Python code block. \n",
      "You don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.<|end|>\n",
      "<|assistant|>\n",
      "**Plan:**\n",
      "1. Open the gripper to ensure it is ready to grasp objects.\n",
      "2. Get the graspable point and size of BallG.\n",
      "3. Move to a position directly above BallG.\n",
      "4. Move down to the graspable point of BallG.\n",
      "5. Close the gripper to grasp BallG.\n",
      "6. Get the size and center position of BoxH.\n",
      "7. Move to a position directly above the center of BoxH.\n",
      "8. Move down to the center of BoxH.\n",
      "9. Open the gripper to release BallG in the center of BoxH.\n",
      "10. Move the gripper away from BoxH to complete the task.\n",
      "**Python Code:**\n",
      "python\n",
      "import numpy as np  # Import numpy because we are using it below\n",
      "# Step 1: Open the gripper\n",
      "open_gripper()\n",
      "# Step 2: Get the graspable point and size of BallG\n",
      "ballG_graspable_point = get_graspable_point('BallG')\n",
      "ballG_size = get_size('BallG')\n",
      "# Step 3: Move to a position directly above BallG\n",
      "above_ballG = ballG_graspable_point + np.array([0, 0, ballG_size[2] + 0.1])  # Adding a buffer of 0.1 units above BallG\n",
      "move_to_position(above_ballG)\n",
      "# Step 4: Move down to the graspable point of BallG\n",
      "move_to_position(ballG_graspable_point)\n",
      "# Step 5: Close the gripper to grasp BallG\n",
      "close_gripper()\n",
      "# Step 6: Get the size and center position of BoxH\n",
      "boxH_size = get_size('BoxH')\n",
      "center_boxH = get_graspable_point('BoxH')\n",
      "center_boxH[0] += boxH_size[0] / 2\n",
      "center_boxH[1] += boxH_size[1] / 2\n",
      "# Step 7: Move to a position directly above the center of BoxH\n",
      "above_center_boxH = center_boxH + np.array([0, 0, boxH_size[2] + ballG_size[2] + 0.1])  # Adding a buffer of 0.1 units above BoxH\n",
      "move_to_position(above_center_boxH)\n",
      "# Step 8: Move down to the center of BoxH\n",
      "move_to_position(center_boxH)\n",
      "# Step 9: Open the gripper to release BallG in the center of BoxH\n",
      "open_gripper()\n",
      "# Step 10: Move the gripper away from BoxH to complete the task\n",
      "move_to_position(above_center_boxH)\n",
      "<|end|>\n"
     ]
    }
   ],
   "source": [
    "print(final_dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 337 examples [00:00, 593.66 examples/s]\n",
      "Generating train split: 28 examples [00:00, 635.53 examples/s]\n",
      "/home/et/miniconda3/envs/llavaft/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 337\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Training with DataParallel so batch size has been adjusted to: 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 87\n",
      "  Number of trainable parameters = 1,572,864\n",
      "/home/et/miniconda3/envs/llavaft/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.135500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=final_dataset['train'],\n",
    "    eval_dataset=final_dataset1['train'],\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint_dir\n",
      "tokenizer config file saved in ./checkpoint_dir/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint_dir/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(train_conf.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "output_dir = \"/mnt/datascience1/checkpoint_dir\"\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "\n",
    "# Load the LoRA adapter configuration\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Apply the LoRA adapter to the base model\n",
    "model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens = 1024, do_sample=True, num_beams=1, temperature=0.2, top_k=50, top_p=0.95,\n",
    "                   max_time= 1024)# .to(device) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(test_inference(str(prompt[12:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_v1.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
