{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilun/miniconda3/envs/et_robot/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1049\n",
      "['▁Task', '▁is', '▁stack', '▁Box', 'I', '▁on', '▁top', '▁of', '▁Ball', 'J', '<0x0A>', 'You', '▁are', '▁a', '▁robot', 'ic', '▁arm', '▁with', '▁advanced', '▁planning', '▁capabilities', '.', '▁Your', '▁task', '▁is', '▁to', '▁generate', '▁Python', '▁code', '▁using', '▁parameter', 'ized', '▁skills', '▁(', 'open', '_', 'g', 'ri', 'pper', '(),', '▁close', '_', 'g', 'ri', 'pper', '(),', '▁move', '_', 'to', '_', 'position', '(),', '▁get', '_', 'gr', 'asp', 'able', '_', 'point', '(),', '▁get', '_', 'size', '())', '▁that', '▁accomplish', 'es', '▁the', '▁user', \"'\", 's', '▁specified', '▁task', '.', '<0x0A>', 'Please', '▁produce', '▁executable', '▁Python', '▁code', '▁that', '▁emp', 'lo', 'ys', '▁these', '▁pre', '-', 'script', 'ed', '▁parameter', 'ized', '▁skills', '.', '▁Remember', '▁to', '▁import', '▁the', '▁necessary', '▁package', '▁before', '▁running', '▁the', '▁code', '.', '▁Care', 'fully', '▁think', '▁through', '▁your', '▁plans', '▁and', '▁code', '.', '<0x0A>', 'When', '▁generating', '▁plans', ',', '▁consider', '▁spatial', '▁relationships', '▁met', 'icul', 'ously', '.', '▁', '<0x0A>', 'For', '▁example', ':', '▁If', '▁you', '▁need', '▁to', '▁pick', '▁up', '▁an', '▁object', ',', '▁first', '▁move', '▁to', '▁a', '▁position', '▁above', '▁it', ',', '▁then', '▁move', '▁down', '▁to', '▁grasp', '▁it', '.', '▁Mov', 'ing', '▁directly', '▁to', '▁the', '▁object', \"'\", 's', '▁position', '▁may', '▁push', '▁it', '▁away', '.', '▁Tre', 'at', '▁it', '▁as', '▁a', '▁two', '-', 'step', '▁process', '.', '▁After', '▁this', ',', '▁consider', '▁whether', '▁the', '▁g', 'ri', 'pper', '▁might', '▁hit', '▁another', '▁object', '▁while', '▁moving', '▁to', '▁the', '▁next', '▁position', '.', '<0x0A>', 'Here', '▁is', '▁an', '▁example', '▁snippet', '▁for', '▁your', '▁reference', ',', '▁demonstr', 'ating', '▁how', '▁to', '▁call', '▁the', '▁function', ':', '<0x0A>', '\"\"', '<0x0A>', 'python', '<0x0A>', 'import', '▁numpy', '▁as', '▁np', '▁', '▁#', '▁import', '▁numpy', '▁because', '▁we', '▁are', '▁using', '▁it', '▁below', '<0x0A>', 'open', '_', 'g', 'ri', 'pper', '()', '<0x0A>', 'close', '_', 'g', 'ri', 'pper', '()', '<0x0A>', '#', '▁Get', '▁the', '▁grasp', 'able', '▁point', '▁of', '▁cube', 'A', '<0x0A>', 'c', 'ube', 'A', '_', 'gr', 'asp', 'able', '_', 'point', '▁=', '▁get', '_', 'gr', 'asp', 'able', '_', 'point', \"('\", 'c', 'ube', 'A', \"')\", '<0x0A>', '#', '▁Get', '▁size', '▁of', '▁cube', 'A', '<0x0A>', 'c', 'ube', 'A', '_', 'size', '▁=', '▁get', '_', 'size', \"('\", 'c', 'ube', 'A', \"')\", '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'c', 'ube', 'A', '_', 'gr', 'asp', 'able', '_', 'point', ')', '<0x0A>', '\"\"', '<0x0A>', 'Please', '▁generate', '▁a', '▁step', '-', 'by', '-', 'step', '▁plan', '▁followed', '▁by', '▁a', '▁single', '▁Python', '▁code', '▁block', '.', '▁', '<0x0A>', 'You', '▁don', \"'\", 't', '▁need', '▁to', '▁define', '▁all', '▁the', '▁functions', '▁again', ',', '▁just', '▁use', '▁them', '.', '▁There', \"'\", 's', '▁no', '▁need', '▁to', '▁define', '▁a', '▁function', '▁for', '▁the', '▁task', ',', '▁just', '▁generate', '▁the', '▁code', '.', '<|end|>\\n', '<|assistant|>\\n', '▁**', 'Plan', ':', '**', '<0x0A>', '1', '.', '▁Open', '▁the', '▁g', 'ri', 'pper', '▁to', '▁ensure', '▁it', '▁is', '▁ready', '▁to', '▁grasp', '▁objects', '.', '<0x0A>', '2', '.', '▁Get', '▁the', '▁grasp', 'able', '▁point', '▁and', '▁size', '▁of', '▁Box', 'I', '.', '<0x0A>', '3', '.', '▁Move', '▁to', '▁a', '▁position', '▁directly', '▁above', '▁Box', 'I', '.', '<0x0A>', '4', '.', '▁Move', '▁down', '▁to', '▁the', '▁grasp', 'able', '▁point', '▁of', '▁Box', 'I', '.', '<0x0A>', '5', '.', '▁Close', '▁the', '▁g', 'ri', 'pper', '▁to', '▁grasp', '▁Box', 'I', '.', '<0x0A>', '6', '.', '▁Get', '▁the', '▁grasp', 'able', '▁point', '▁and', '▁size', '▁of', '▁Ball', 'J', '.', '<0x0A>', '7', '.', '▁Move', '▁to', '▁a', '▁position', '▁directly', '▁above', '▁Ball', 'J', '▁while', '▁ens', 'uring', '▁Box', 'I', '▁does', '▁not', '▁col', 'lide', '▁with', '▁Ball', 'J', '.', '<0x0A>', '8', '.', '▁Att', 'empt', '▁to', '▁move', '▁down', '▁to', '▁place', '▁Box', 'I', '▁on', '▁top', '▁of', '▁Ball', 'J', '.', '▁This', '▁will', '▁fail', '▁as', '▁the', '▁ball', '▁is', '▁not', '▁a', '▁stable', '▁surface', '▁for', '▁the', '▁box', '.', '<0x0A>', '9', '.', '▁Open', '▁the', '▁g', 'ri', 'pper', '▁to', '▁release', '▁Box', 'I', '.', '<0x0A>', '1', '0', '.', '▁Move', '▁the', '▁g', 'ri', 'pper', '▁away', '▁from', '▁Ball', 'J', '▁to', '▁complete', '▁the', '▁task', '.', '<0x0A>', '**', 'Python', '▁Code', ':', '**', '<0x0A>', 'python', '<0x0A>', 'import', '▁numpy', '▁as', '▁np', '▁', '▁#', '▁Import', '▁numpy', '▁because', '▁we', '▁are', '▁using', '▁it', '▁below', '<0x0A>', '#', '▁Step', '▁', '1', ':', '▁Open', '▁the', '▁g', 'ri', 'pper', '<0x0A>', 'open', '_', 'g', 'ri', 'pper', '()', '<0x0A>', '#', '▁Step', '▁', '2', ':', '▁Get', '▁the', '▁grasp', 'able', '▁point', '▁and', '▁size', '▁of', '▁Box', 'I', '<0x0A>', 'box', 'I', '_', 'gr', 'asp', 'able', '_', 'point', '▁=', '▁get', '_', 'gr', 'asp', 'able', '_', 'point', \"('\", 'Box', 'I', \"')\", '<0x0A>', 'box', 'I', '_', 'size', '▁=', '▁get', '_', 'size', \"('\", 'Box', 'I', \"')\", '<0x0A>', '#', '▁Step', '▁', '3', ':', '▁Move', '▁to', '▁a', '▁position', '▁directly', '▁above', '▁Box', 'I', '<0x0A>', 'above', '_', 'box', 'I', '▁=', '▁box', 'I', '_', 'gr', 'asp', 'able', '_', 'point', '▁+', '▁np', '.', 'array', '([', '0', ',', '▁', '0', ',', '▁box', 'I', '_', 'size', '[', '2', ']', '▁+', '▁', '0', '.', '1', '])', '▁', '▁#', '▁Adding', '▁a', '▁buffer', '▁of', '▁', '0', '.', '1', '▁units', '▁above', '▁Box', 'I', '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'above', '_', 'box', 'I', ')', '<0x0A>', '#', '▁Step', '▁', '4', ':', '▁Move', '▁down', '▁to', '▁the', '▁grasp', 'able', '▁point', '▁of', '▁Box', 'I', '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'box', 'I', '_', 'gr', 'asp', 'able', '_', 'point', ')', '<0x0A>', '#', '▁Step', '▁', '5', ':', '▁Close', '▁the', '▁g', 'ri', 'pper', '▁to', '▁grasp', '▁Box', 'I', '<0x0A>', 'close', '_', 'g', 'ri', 'pper', '()', '<0x0A>', '#', '▁Step', '▁', '6', ':', '▁Get', '▁the', '▁grasp', 'able', '▁point', '▁and', '▁size', '▁of', '▁Ball', 'J', '<0x0A>', 'ball', 'J', '_', 'gr', 'asp', 'able', '_', 'point', '▁=', '▁get', '_', 'gr', 'asp', 'able', '_', 'point', \"('\", 'B', 'all', 'J', \"')\", '<0x0A>', 'ball', 'J', '_', 'size', '▁=', '▁get', '_', 'size', \"('\", 'B', 'all', 'J', \"')\", '<0x0A>', '#', '▁Step', '▁', '7', ':', '▁Move', '▁to', '▁a', '▁position', '▁directly', '▁above', '▁Ball', 'J', '<0x0A>', 'above', '_', 'ball', 'J', '▁=', '▁ball', 'J', '_', 'gr', 'asp', 'able', '_', 'point', '▁+', '▁np', '.', 'array', '([', '0', ',', '▁', '0', ',', '▁ball', 'J', '_', 'size', '[', '2', ']', '▁+', '▁box', 'I', '_', 'size', '[', '2', ']', '▁+', '▁', '0', '.', '1', '])', '▁', '▁#', '▁Adding', '▁a', '▁buffer', '▁of', '▁', '0', '.', '1', '▁units', '▁above', '▁Ball', 'J', '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'above', '_', 'ball', 'J', ')', '<0x0A>', '#', '▁Step', '▁', '8', ':', '▁Att', 'empt', '▁to', '▁move', '▁down', '▁to', '▁place', '▁Box', 'I', '▁on', '▁top', '▁of', '▁Ball', 'J', '▁(', 'this', '▁will', '▁fail', ')', '<0x0A>', 'just', '_', 'above', '_', 'ball', 'J', '▁=', '▁ball', 'J', '_', 'gr', 'asp', 'able', '_', 'point', '▁+', '▁np', '.', 'array', '([', '0', ',', '▁', '0', ',', '▁ball', 'J', '_', 'size', '[', '2', ']', '▁+', '▁box', 'I', '_', 'size', '[', '2', ']])', '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'just', '_', 'above', '_', 'ball', 'J', ')', '<0x0A>', '#', '▁Step', '▁', '9', ':', '▁Open', '▁the', '▁g', 'ri', 'pper', '▁to', '▁release', '▁Box', 'I', '<0x0A>', 'open', '_', 'g', 'ri', 'pper', '()', '<0x0A>', '#', '▁Step', '▁', '1', '0', ':', '▁Move', '▁the', '▁g', 'ri', 'pper', '▁away', '▁from', '▁Ball', 'J', '▁to', '▁complete', '▁the', '▁task', '<0x0A>', 'move', '_', 'to', '_', 'position', '(', 'above', '_', 'ball', 'J', ')', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the Phi-3-mini model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Example input text\n",
    "input_text = \"Task is stack BoxI on top of BallJ\\nYou are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\\nPlease produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\\nWhen generating plans, consider spatial relationships meticulously. \\nFor example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\\nHere is an example snippet for your reference, demonstrating how to call the function:\\n\\\"\\\"\\npython\\nimport numpy as np  # import numpy because we are using it below\\nopen_gripper()\\nclose_gripper()\\n# Get the graspable point of cubeA\\ncubeA_graspable_point = get_graspable_point('cubeA')\\n# Get size of cubeA\\ncubeA_size = get_size('cubeA')\\nmove_to_position(cubeA_graspable_point)\\n\\\"\\\"\\nPlease generate a step-by-step plan followed by a single Python code block. \\nYou don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.<|end|>\\n<|assistant|>\\n**Plan:**\\n1. Open the gripper to ensure it is ready to grasp objects.\\n2. Get the graspable point and size of BoxI.\\n3. Move to a position directly above BoxI.\\n4. Move down to the graspable point of BoxI.\\n5. Close the gripper to grasp BoxI.\\n6. Get the graspable point and size of BallJ.\\n7. Move to a position directly above BallJ while ensuring BoxI does not collide with BallJ.\\n8. Attempt to move down to place BoxI on top of BallJ. This will fail as the ball is not a stable surface for the box.\\n9. Open the gripper to release BoxI.\\n10. Move the gripper away from BallJ to complete the task.\\n**Python Code:**\\npython\\nimport numpy as np  # Import numpy because we are using it below\\n# Step 1: Open the gripper\\nopen_gripper()\\n# Step 2: Get the graspable point and size of BoxI\\nboxI_graspable_point = get_graspable_point('BoxI')\\nboxI_size = get_size('BoxI')\\n# Step 3: Move to a position directly above BoxI\\nabove_boxI = boxI_graspable_point + np.array([0, 0, boxI_size[2] + 0.1])  # Adding a buffer of 0.1 units above BoxI\\nmove_to_position(above_boxI)\\n# Step 4: Move down to the graspable point of BoxI\\nmove_to_position(boxI_graspable_point)\\n# Step 5: Close the gripper to grasp BoxI\\nclose_gripper()\\n# Step 6: Get the graspable point and size of BallJ\\nballJ_graspable_point = get_graspable_point('BallJ')\\nballJ_size = get_size('BallJ')\\n# Step 7: Move to a position directly above BallJ\\nabove_ballJ = ballJ_graspable_point + np.array([0, 0, ballJ_size[2] + boxI_size[2] + 0.1])  # Adding a buffer of 0.1 units above BallJ\\nmove_to_position(above_ballJ)\\n# Step 8: Attempt to move down to place BoxI on top of BallJ (this will fail)\\njust_above_ballJ = ballJ_graspable_point + np.array([0, 0, ballJ_size[2] + boxI_size[2]])\\nmove_to_position(just_above_ballJ)\\n# Step 9: Open the gripper to release BoxI\\nopen_gripper()\\n# Step 10: Move the gripper away from BallJ to complete the task\\nmove_to_position(above_ballJ)\\n\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# Print the number of tokens\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Optionally, print the tokens themselves\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is place CubeA on top of CubeB\n",
      "\n",
      "You are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\n",
      "Please produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\n",
      "When generating plans, consider spatial relationships meticulously. \n",
      "For example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\n",
      "Here is an example snippet for your reference, demonstrating how to call the function:\n",
      "\"\"\"\n",
      "python\n",
      "import numpy as np  # import numpy because we are using it below\n",
      "\n",
      "open_gripper()\n",
      "close_gripper()\n",
      "# Get the graspable point of cubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Get size of cubeA\n",
      "cubeA_size = get_size('cubeA')\n",
      "\n",
      "move_to_position(cubeA_graspable_point)\n",
      "\"\"\"\n",
      "\n",
      "Please generate a step-by-step plan followed by a single Python code block. \n",
      "You don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.\n",
      "\n",
      "\n",
      " Step-by-step plan:\n",
      "\n",
      "1. Open the gripper to prepare for picking up CubeA.\n",
      "2. Move to a position above CubeB.\n",
      "3. Get the graspable point of CubeA.\n",
      "4. Close the gripper to grasp CubeA.\n",
      "5. Move to the position above CubeB where CubeA will be placed.\n",
      "6. Open the gripper to release CubeA onto CubeB.\n",
      "\n",
      "Python code:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Step 1: Open the gripper\n",
      "open_gripper()\n",
      "\n",
      "# Step 2: Move to a position above CubeB\n",
      "cubeB_position = get_graspable_point('cubeB')\n",
      "move_to_position(cubeB_position)\n",
      "\n",
      "# Step 3: Get the graspable point of CubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Step 4: Close the gripper to grasp CubeA\n",
      "close_gripper()\n",
      "\n",
      "# Step 5: Move to the position above CubeB where CubeA will be placed\n",
      "place_position = cubeB_position + np.array([0, cubeA_graspable_point[1], cubeA_graspable_point[2]])\n",
      "move_to_position(place_position)\n",
      "\n",
      "# Step 6: Open the gripper to release CubeA onto CubeB\n",
      "open_gripper()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the directory where the fine-tuned LoRA adapters are saved\n",
    "output_dir = \"phi-3-mini-LoRA/checkpoint-725/\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "\n",
    "# Load the LoRA adapter configuration\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Apply the LoRA adapter to the base model\n",
    "# model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "model = model.to(device)\n",
    "\n",
    "# prompt_path = '/home/xilun/ET_robot/prompt_stack_two.txt'\n",
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "## delete <|user|> and <|assistant|> and <|end|> if found in prompt \n",
    "prompt = prompt.replace(\"Starts<\", \"\").replace(\">Ends\", \"\")\n",
    "prompt = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").replace(\"<|end|>\", \"\")\n",
    "print(prompt)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print (inputs)\n",
    "# input()\n",
    "# prompt = prompt + \"<|assistant|>\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "# Generate text using the model's generate method\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0.2,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": False,\n",
    "    # \"top_k\": 50,\n",
    "    # \"top_p\": 0.95\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate the output\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"],\n",
    "#     max_new_tokens=generation_args[\"max_new_tokens\"],\n",
    "#     temperature=generation_args[\"temperature\"],\n",
    "#     do_sample=generation_args[\"do_sample\"],\n",
    "#     # top_k=generation_args[\"top_k\"],\n",
    "#     # top_p=generation_args[\"top_p\"],\n",
    "#     # pad_token_id=tokenizer.pad_token_id,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# # Decode the generated output with chat template\n",
    "\n",
    "\n",
    "\n",
    "# output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(output_text)\n",
    "del model\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>Starts<\n",
      "\n",
      "Task is place CubeA on top of CubeB\n",
      "\n",
      "You are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\n",
      "Please produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\n",
      "When generating plans, consider spatial relationships meticulously. \n",
      "For example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\n",
      "Here is an example snippet for your reference, demonstrating how to call the function:\n",
      "\"\"\"\n",
      "python\n",
      "import numpy as np  # import numpy because we are using it below\n",
      "\n",
      "open_gripper()\n",
      "close_gripper()\n",
      "# Get the graspable point of cubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Get size of cubeA\n",
      "cubeA_size = get_size('cubeA')\n",
      "\n",
      "move_to_position(cubeA_graspable_point)\n",
      "\"\"\"\n",
      "\n",
      "Please generate a step-by-step plan followed by a single Python code block. \n",
      "You don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.\n",
      "\n",
      ">Ends<|end|>\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the directory where the fine-tuned LoRA adapters are saved\n",
    "output_dir = \"phi-3-mini-LoRA/checkpoint-1674/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "new_model = new_model.to(device)\n",
    "new_model = new_model.merge_and_unload()\n",
    "# prompt_path = '/home/xilun/ET_robot/prompt_stack_two.txt'\n",
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "## delete <|user|> and <|assistant|> and <|end|> if found in prompt \n",
    "prompt = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").replace(\"<|end|>\", \"\")\n",
    "\n",
    "\n",
    "# Generate text using the model's generate method\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": True,\n",
    "    # \"top_k\": 50,\n",
    "    # \"top_p\": 0.95\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate the output\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=new_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=1500, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "print (outputs[0]['generated_text'][len(prompt):].strip())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_robot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
