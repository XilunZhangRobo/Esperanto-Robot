{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is place CubeA on top of CubeB\n",
      "\n",
      "You are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\n",
      "Please produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\n",
      "When generating plans, consider spatial relationships meticulously. \n",
      "For example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\n",
      "Here is an example snippet for your reference, demonstrating how to call the function:\n",
      "\"\"\"\n",
      "python\n",
      "import numpy as np  # import numpy because we are using it below\n",
      "\n",
      "open_gripper()\n",
      "close_gripper()\n",
      "# Get the graspable point of cubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Get size of cubeA\n",
      "cubeA_size = get_size('cubeA')\n",
      "\n",
      "move_to_position(cubeA_graspable_point)\n",
      "\"\"\"\n",
      "\n",
      "Please generate a step-by-step plan followed by a single Python code block. \n",
      "You don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.\n",
      "\n",
      "\n",
      " Step-by-step plan:\n",
      "\n",
      "1. Open the gripper to prepare for picking up CubeA.\n",
      "2. Move to a position above CubeB.\n",
      "3. Get the graspable point of CubeA.\n",
      "4. Close the gripper to grasp CubeA.\n",
      "5. Move to the position above CubeB where CubeA will be placed.\n",
      "6. Open the gripper to release CubeA onto CubeB.\n",
      "\n",
      "Python code:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Step 1: Open the gripper\n",
      "open_gripper()\n",
      "\n",
      "# Step 2: Move to a position above CubeB\n",
      "cubeB_position = get_graspable_point('cubeB')\n",
      "move_to_position(cubeB_position)\n",
      "\n",
      "# Step 3: Get the graspable point of CubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Step 4: Close the gripper to grasp CubeA\n",
      "close_gripper()\n",
      "\n",
      "# Step 5: Move to the position above CubeB where CubeA will be placed\n",
      "place_position = cubeB_position + np.array([0, cubeA_graspable_point[1], cubeA_graspable_point[2]])\n",
      "move_to_position(place_position)\n",
      "\n",
      "# Step 6: Open the gripper to release CubeA onto CubeB\n",
      "open_gripper()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the directory where the fine-tuned LoRA adapters are saved\n",
    "output_dir = \"phi-3-mini-LoRA/checkpoint-725/\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "\n",
    "# Load the LoRA adapter configuration\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Apply the LoRA adapter to the base model\n",
    "# model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "model = model.to(device)\n",
    "\n",
    "# prompt_path = '/home/xilun/ET_robot/prompt_stack_two.txt'\n",
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "## delete <|user|> and <|assistant|> and <|end|> if found in prompt \n",
    "prompt = prompt.replace(\"Starts<\", \"\").replace(\">Ends\", \"\")\n",
    "prompt = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").replace(\"<|end|>\", \"\")\n",
    "print(prompt)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print (inputs)\n",
    "# input()\n",
    "# prompt = prompt + \"<|assistant|>\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "# Generate text using the model's generate method\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0.2,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": False,\n",
    "    # \"top_k\": 50,\n",
    "    # \"top_p\": 0.95\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate the output\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"],\n",
    "#     max_new_tokens=generation_args[\"max_new_tokens\"],\n",
    "#     temperature=generation_args[\"temperature\"],\n",
    "#     do_sample=generation_args[\"do_sample\"],\n",
    "#     # top_k=generation_args[\"top_k\"],\n",
    "#     # top_p=generation_args[\"top_p\"],\n",
    "#     # pad_token_id=tokenizer.pad_token_id,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# # Decode the generated output with chat template\n",
    "\n",
    "\n",
    "\n",
    "# output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(output_text)\n",
    "del model\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>Starts<\n",
      "\n",
      "Task is place CubeA on top of CubeB\n",
      "\n",
      "You are a robotic arm with advanced planning capabilities. Your task is to generate Python code using parameterized skills (open_gripper(), close_gripper(), move_to_position(), get_graspable_point(), get_size()) that accomplishes the user's specified task.\n",
      "Please produce executable Python code that employs these pre-scripted parameterized skills. Remember to import the necessary package before running the code. Carefully think through your plans and code.\n",
      "When generating plans, consider spatial relationships meticulously. \n",
      "For example: If you need to pick up an object, first move to a position above it, then move down to grasp it. Moving directly to the object's position may push it away. Treat it as a two-step process. After this, consider whether the gripper might hit another object while moving to the next position.\n",
      "Here is an example snippet for your reference, demonstrating how to call the function:\n",
      "\"\"\"\n",
      "python\n",
      "import numpy as np  # import numpy because we are using it below\n",
      "\n",
      "open_gripper()\n",
      "close_gripper()\n",
      "# Get the graspable point of cubeA\n",
      "cubeA_graspable_point = get_graspable_point('cubeA')\n",
      "\n",
      "# Get size of cubeA\n",
      "cubeA_size = get_size('cubeA')\n",
      "\n",
      "move_to_position(cubeA_graspable_point)\n",
      "\"\"\"\n",
      "\n",
      "Please generate a step-by-step plan followed by a single Python code block. \n",
      "You don't need to define all the functions again, just use them. There's no need to define a function for the task, just generate the code.\n",
      "\n",
      ">Ends<|end|>\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the directory where the fine-tuned LoRA adapters are saved\n",
    "output_dir = \"phi-3-mini-LoRA/checkpoint-1674/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "new_model = new_model.to(device)\n",
    "new_model = new_model.merge_and_unload()\n",
    "# prompt_path = '/home/xilun/ET_robot/prompt_stack_two.txt'\n",
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[0][\"input\"]\n",
    "## delete <|user|> and <|assistant|> and <|end|> if found in prompt \n",
    "prompt = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").replace(\"<|end|>\", \"\")\n",
    "\n",
    "\n",
    "# Generate text using the model's generate method\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": True,\n",
    "    # \"top_k\": 50,\n",
    "    # \"top_p\": 0.95\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate the output\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=new_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "outputs = pipe(prompt, max_new_tokens=1500, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "print (outputs[0]['generated_text'][len(prompt):].strip())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_robot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
