{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilun/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-13 18:20:39,826] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilun/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-70B-Instruct is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(url\u001b[39m=\u001b[39;49murl, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m   1723\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1646\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1647\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1648\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1649\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1650\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1651\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1652\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1653\u001b[0m )\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    373\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    374\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    375\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    376\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    379\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    397\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    361\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Forbidden: \u001b[39m\u001b[39m{\u001b[39;00merror_message\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCannot access content at: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf you are trying to create or update content,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmake sure you have a token with the `write` role.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: Root=1-666b9ae8-118ecbcd4e35bc5131253cbf;83b798ae-7264-423c-95bb-47587e25663a)\n\n403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..\nCannot access content at: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/resolve/main/config.json.\nIf you are trying to create or update content,make sure you have a token with the `write` role.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    386\u001b[0m         path_or_repo_id,\n\u001b[1;32m    387\u001b[0m         filename,\n\u001b[1;32m    388\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    389\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    390\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    391\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    392\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    393\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    394\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    395\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    396\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    397\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1222\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1224\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1226\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1227\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1228\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1229\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1231\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1232\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1233\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1234\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1236\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1237\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[39m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1327\u001b[0m \u001b[39m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1826\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1826\u001b[0m     \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1827\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1829\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1830\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mhead_call_error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/xilun/ET_robot/LLaMA3.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Meta-Llama-3-70B-Instruct\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtorch_dtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: torch\u001b[39m.\u001b[39;49mbfloat16},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mYou are a pirate chatbot who always responds in pirate speak!\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWho are you?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m terminators \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     pipeline\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     pipeline\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(\u001b[39m\"\u001b[39m\u001b[39m<|eot_id|>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227361666561692d677075352e6c616e2e6c6f63616c2e636d752e656475222c2275736572223a2278696c756e227d/home/xilun/ET_robot/LLaMA3.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/pipelines/__init__.py:782\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m                 adapter_config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    780\u001b[0m                 model \u001b[39m=\u001b[39m adapter_config[\u001b[39m\"\u001b[39m\u001b[39mbase_model_name_or_path\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 782\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    783\u001b[0m         model, _from_pipeline\u001b[39m=\u001b[39;49mtask, code_revision\u001b[39m=\u001b[39;49mcode_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    784\u001b[0m     )\n\u001b[1;32m    785\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    787\u001b[0m custom_tasks \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/et_robot/lib/python3.8/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[1;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-70B-Instruct is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```python\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "# Step 1: Open the gripper to prepare for picking up the ball\n",
      "\n",
      "open_gripper()\n",
      "\n",
      "\n",
      "# Step 2: Move to the position above CubeC to avoid pushing it away\n",
      "\n",
      "cubeC_position = get_position('cubeC')  # Assuming get_position() returns the position of CubeC\n",
      "\n",
      "move_to_position(cubeC_position + np.array([0, cubeC_position[1], cubeC_position[2]]))\n",
      "\n",
      "\n",
      "# Step 3: Get the graspable point of BallA\n",
      "\n",
      "ballA_graspable_point = get_graspable_point('ballA')\n",
      "\n",
      "\n",
      "# Step 4: Close the gripper to grasp BallA\n",
      "\n",
      "close_gripper()\n",
      "\n",
      "\n",
      "# Step 5: Move to the position above CubeC where BallA will be placed\n",
      "\n",
      "move_to_position(cubeC_position + np.array([0, cubeC_position[1], cubeC_position[2] + ballA_graspable_point[2] - cubeC_size[2] / 2]))\n",
      "\n",
      "\n",
      "# Step 6: Open the gripper to release BallA onto CubeC\n",
      "\n",
      "open_gripper()\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Ensure the device is set correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the directory where the fine-tuned LoRA adapters are saved\n",
    "output_dir = \"phi-3-mini-LoRA/checkpoint-1097/\"\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "\n",
    "# Load the LoRA adapter configuration\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Apply the LoRA adapter to the base model\n",
    "# model = PeftModel.from_pretrained(model, output_dir, torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "model = model.to(device)\n",
    "\n",
    "# prompt_path = '/home/xilun/ET_robot/prompt_stack_two.txt'\n",
    "prompt_path = \"/home/xilun/ET_robot/dataset/172_dataset.json\"\n",
    "# Load the prompt as json file\n",
    "prompt = json.loads(open(prompt_path).read())\n",
    "prompt = prompt[1][\"input\"]\n",
    "## delete <|user|> and <|assistant|> and <|end|> if found in prompt \n",
    "# prompt = prompt.replace(\"<|user|>\", \"\").replace(\"<|assistant|>\", \"\").replace(\"<|end|>\", \"\")\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# prompt = prompt + \"Please generate the plan\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "# Generate text using the model's generate method\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "    \"return_full_text\": False,\n",
    "    # \"top_k\": 50,\n",
    "    # \"top_p\": 0.95\n",
    "    \n",
    "}\n",
    "\n",
    "# Generate the output\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n",
    "\n",
    "\n",
    "\n",
    "# output = model.generate(\n",
    "#     input_ids=inputs[\"input_ids\"],\n",
    "#     max_new_tokens=generation_args[\"max_new_tokens\"],\n",
    "#     temperature=generation_args[\"temperature\"],\n",
    "#     do_sample=generation_args[\"do_sample\"],\n",
    "#     # top_k=generation_args[\"top_k\"],\n",
    "#     # top_p=generation_args[\"top_p\"],\n",
    "#     # pad_token_id=tokenizer.pad_token_id,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# # Decode the generated output with chat template\n",
    "\n",
    "\n",
    "\n",
    "# output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(output_text)\n",
    "del model\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_robot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
